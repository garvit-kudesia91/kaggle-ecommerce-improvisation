{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kaggle ecommerce challenge (Improvisation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas_profiling\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Clothing ID</th>\n",
       "      <th>Age</th>\n",
       "      <th>Title</th>\n",
       "      <th>Review Text</th>\n",
       "      <th>Rating</th>\n",
       "      <th>Recommended IND</th>\n",
       "      <th>Positive Feedback Count</th>\n",
       "      <th>Division Name</th>\n",
       "      <th>Department Name</th>\n",
       "      <th>Class Name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>767</td>\n",
       "      <td>33</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Absolutely wonderful - silky and sexy and comf...</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Initmates</td>\n",
       "      <td>Intimate</td>\n",
       "      <td>Intimates</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1080</td>\n",
       "      <td>34</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Love this dress!  it's sooo pretty.  i happene...</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>General</td>\n",
       "      <td>Dresses</td>\n",
       "      <td>Dresses</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1077</td>\n",
       "      <td>60</td>\n",
       "      <td>Some major design flaws</td>\n",
       "      <td>I had such high hopes for this dress and reall...</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>General</td>\n",
       "      <td>Dresses</td>\n",
       "      <td>Dresses</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>1049</td>\n",
       "      <td>50</td>\n",
       "      <td>My favorite buy!</td>\n",
       "      <td>I love, love, love this jumpsuit. it's fun, fl...</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>General Petite</td>\n",
       "      <td>Bottoms</td>\n",
       "      <td>Pants</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>847</td>\n",
       "      <td>47</td>\n",
       "      <td>Flattering shirt</td>\n",
       "      <td>This shirt is very flattering to all due to th...</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>General</td>\n",
       "      <td>Tops</td>\n",
       "      <td>Blouses</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  Clothing ID  Age                    Title  \\\n",
       "0           0          767   33                      NaN   \n",
       "1           1         1080   34                      NaN   \n",
       "2           2         1077   60  Some major design flaws   \n",
       "3           3         1049   50         My favorite buy!   \n",
       "4           4          847   47         Flattering shirt   \n",
       "\n",
       "                                         Review Text  Rating  Recommended IND  \\\n",
       "0  Absolutely wonderful - silky and sexy and comf...       4                1   \n",
       "1  Love this dress!  it's sooo pretty.  i happene...       5                1   \n",
       "2  I had such high hopes for this dress and reall...       3                0   \n",
       "3  I love, love, love this jumpsuit. it's fun, fl...       5                1   \n",
       "4  This shirt is very flattering to all due to th...       5                1   \n",
       "\n",
       "   Positive Feedback Count   Division Name Department Name Class Name  \n",
       "0                        0       Initmates        Intimate  Intimates  \n",
       "1                        4         General         Dresses    Dresses  \n",
       "2                        0         General         Dresses    Dresses  \n",
       "3                        0  General Petite         Bottoms      Pants  \n",
       "4                        6         General            Tops    Blouses  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('Womens Clothing E-Commerce Reviews.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from stop_words import get_stop_words\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import sent_tokenize, word_tokenize\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "top_N = 100\n",
    "#convert list of list into text\n",
    "\n",
    "a = df['Review Text'].str.lower().str.cat(sep=' ')\n",
    "\n",
    "# removes punctuation,numbers and returns list of words\n",
    "b = re.sub('[^A-Za-z]+', ' ', a)\n",
    "\n",
    "#remove all the stopwords from the text\n",
    "stop_words = list(get_stop_words('en'))         \n",
    "nltk_words = list(stopwords.words('english'))   \n",
    "stop_words.extend(nltk_words)\n",
    "\n",
    "word_tokens = word_tokenize(b)\n",
    "filtered_sentence = [w for w in word_tokens if not w in stop_words]\n",
    "filtered_sentence = []\n",
    "for w in word_tokens:\n",
    "    if w not in stop_words:\n",
    "        filtered_sentence.append(w)\n",
    "\n",
    "# Remove characters which have length less than 2  \n",
    "without_single_chr = [word for word in filtered_sentence if len(word) > 2]\n",
    "\n",
    "# Remove numbers\n",
    "cleaned_data_title = [word for word in without_single_chr if not word.isnumeric()]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df.dropna(axis=0,how='any')\n",
    "rating_class = df[(df['Rating'] == 1) | (df['Rating'] == 5)]\n",
    "X_review=rating_class['Review Text']\n",
    "y=rating_class['Rating']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "def text_process(review):\n",
    "    nopunc=[word for word in review if word not in string.punctuation]\n",
    "    nopunc=''.join(nopunc)\n",
    "    return [word for word in nopunc.split() if word.lower() not in stopwords.words('english')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now there are two things, I would like to see here in order to increase the in sample accuracy and precision - recall\n",
    "\n",
    "1) Perform hyperparamater tuning - The only hyperparamter that comes to mind while tuning naive bayes classifier is smoothing or alpha which is by default set at 1 (add one smoothing). However, at no smoothing  (i.e. alpha = 0), you are overfitting the model. So the value of alpha has to be carefully chosen\n",
    "\n",
    "2) Look into a different technique of vectorization which is called TF IDF Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "bow_transformer=CountVectorizer(analyzer=text_process).fit(X_review)\n",
    "X_review = bow_transformer.transform(X_review)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multinomial Naive Bayes Classifier "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9706828302820386"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_review, y, test_size=0.3, random_state=101)\n",
    "nb = MultinomialNB()\n",
    "nb.fit(X_train, y_train) \n",
    "nb.score(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  65  134]\n",
      " [  14 3252]]\n",
      "\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          1       0.82      0.33      0.47       199\n",
      "          5       0.96      1.00      0.98      3266\n",
      "\n",
      "avg / total       0.95      0.96      0.95      3465\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predict = nb.predict(X_test)\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "print(confusion_matrix(y_test, predict))\n",
    "print('\\n')\n",
    "print(classification_report(y_test, predict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparamter tuning for Naive Bayes (Laplace smoothing factor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "parameters = {'alpha':[i/10 for i in range(1,10)]}\n",
    "gridsearch = GridSearchCV(nb,param_grid = parameters,scoring='accuracy',cv=10)\n",
    "gridsearch = gridsearch.fit(X_train, y_train) \n",
    "best_accuracy = gridsearch.best_score_\n",
    "best_paramters = gridsearch.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'alpha': 0.3}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_paramters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.989732805541811"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb = MultinomialNB(alpha=best_paramters['alpha'])\n",
    "nb.fit(X_train, y_train) \n",
    "nb.score(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 122   77]\n",
      " [  63 3203]]\n",
      "\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          1       0.66      0.61      0.64       199\n",
      "          5       0.98      0.98      0.98      3266\n",
      "\n",
      "avg / total       0.96      0.96      0.96      3465\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predict = nb.predict(X_test)\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "print(confusion_matrix(y_test, predict))\n",
    "print('\\n')\n",
    "print(classification_report(y_test, predict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hence, there is an increase in precision by 1%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF IDF Vectorization technique with Multinomial Naive Bayes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_N = 100\n",
    "#convert list of list into text\n",
    "\n",
    "a = df['Review Text'].str.lower().str.cat(sep=' ')\n",
    "\n",
    "# removes punctuation,numbers and returns list of words\n",
    "b = re.sub('[^A-Za-z]+', ' ', a)\n",
    "\n",
    "#remove all the stopwords from the text\n",
    "stop_words = list(get_stop_words('en'))         \n",
    "nltk_words = list(stopwords.words('english'))   \n",
    "stop_words.extend(nltk_words)\n",
    "\n",
    "word_tokens = word_tokenize(b)\n",
    "filtered_sentence = [w for w in word_tokens if not w in stop_words]\n",
    "filtered_sentence = []\n",
    "for w in word_tokens:\n",
    "    if w not in stop_words:\n",
    "        filtered_sentence.append(w)\n",
    "\n",
    "# Remove characters which have length less than 2  \n",
    "without_single_chr = [word for word in filtered_sentence if len(word) > 2]\n",
    "\n",
    "# Remove numbers\n",
    "cleaned_data_title = [word for word in without_single_chr if not word.isnumeric()]  \n",
    "df=df.dropna(axis=0,how='any')\n",
    "rating_class = df[(df['Rating'] == 1) | (df['Rating'] == 5)]\n",
    "X_review=rating_class['Review Text']\n",
    "y=rating_class['Rating']\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "bow_transformer=TfidfVectorizer(analyzer=text_process).fit(X_review)\n",
    "X_review = bow_transformer.transform(X_review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9391390400791687"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_review, y, test_size=0.3, random_state=101)\n",
    "nb = MultinomialNB()\n",
    "nb.fit(X_train, y_train) \n",
    "nb.score(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   0  199]\n",
      " [   0 3266]]\n",
      "\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          1       0.00      0.00      0.00       199\n",
      "          5       0.94      1.00      0.97      3266\n",
      "\n",
      "avg / total       0.89      0.94      0.91      3465\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predict = nb.predict(X_test)\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "print(confusion_matrix(y_test, predict))\n",
    "print('\\n')\n",
    "print(classification_report(y_test, predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "parameters = {'alpha':[i/10 for i in range(1,10)]}\n",
    "gridsearch = GridSearchCV(nb,param_grid = parameters,scoring='accuracy',cv=10)\n",
    "gridsearch = gridsearch.fit(X_train, y_train) \n",
    "best_accuracy = gridsearch.best_score_\n",
    "best_paramters = gridsearch.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9701880257298368"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_review, y, test_size=0.3, random_state=101)\n",
    "nb = MultinomialNB(alpha = best_paramters['alpha'])\n",
    "nb.fit(X_train, y_train) \n",
    "nb.score(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  44  155]\n",
      " [   9 3257]]\n",
      "\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          1       0.83      0.22      0.35       199\n",
      "          5       0.95      1.00      0.98      3266\n",
      "\n",
      "avg / total       0.95      0.95      0.94      3465\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predict = nb.predict(X_test)\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "print(confusion_matrix(y_test, predict))\n",
    "print('\\n')\n",
    "print(classification_report(y_test, predict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hence, at its best smoothing parameter, this technique is able to achieve 95% of precision and recall, which is lower than what we got for Count Vectorizer technique as demonstrated above\n",
    "\n",
    "#### Frankly speaking, TF IFD vectorization is not a good match with Naive Bayes because the primary assumption of multinomial NB classifer is - All features are independent to each other but TF IFD violates that "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (1) Linear Support Vector Classifier with TF IDF Vectorized data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9909698169223157"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_review, y, test_size=0.3, random_state=101)\n",
    "from sklearn.svm import SVC\n",
    "svc = SVC(kernel='linear')\n",
    "svc.fit(X_train, y_train) \n",
    "svc.score(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  98  101]\n",
      " [   8 3258]]\n",
      "\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          1       0.92      0.49      0.64       199\n",
      "          5       0.97      1.00      0.98      3266\n",
      "\n",
      "avg / total       0.97      0.97      0.96      3465\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predict = svc.predict(X_test)\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "print(confusion_matrix(y_test, predict))\n",
    "print('\\n')\n",
    "print(classification_report(y_test, predict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's perform hyperparameter tuning for the penalty factor of support vector classfier and then use the best penalty factor (C) for our prediction "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "parameters = {'C':[i/10 for i in range(1,10)]}\n",
    "gridsearch = GridSearchCV(svc,param_grid = parameters,scoring='accuracy',cv=10)\n",
    "gridsearch = gridsearch.fit(X_train, y_train) \n",
    "best_accuracy = gridsearch.best_score_\n",
    "best_paramters = gridsearch.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'C': 0.9}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_paramters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9889905987135081"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_review, y, test_size=0.3, random_state=101)\n",
    "from sklearn.svm import SVC\n",
    "svc = SVC(kernel='linear',C=best_paramters['C'])\n",
    "svc.fit(X_train, y_train) \n",
    "svc.score(X_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### This means there is no overfitting as such"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  91  108]\n",
      " [   8 3258]]\n",
      "\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          1       0.92      0.46      0.61       199\n",
      "          5       0.97      1.00      0.98      3266\n",
      "\n",
      "avg / total       0.97      0.97      0.96      3465\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predict = svc.predict(X_test)\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "print(confusion_matrix(y_test, predict))\n",
    "print('\\n')\n",
    "print(classification_report(y_test, predict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We get 2% better precision and 1% better recall than Multinomial Naive Bayes Classifier \n",
    "\n",
    "#### I tried count vectorization with SVM and was getting precision - recall of 96%. So, TF IFD with SVM is a better match for this data \n",
    "\n",
    "#### This tells us that default penalty parameter (C=1.0) was restricting the classification by unnecessary shrinking of the margin between hyperplanes. We dropped the penalty by 0.1 and received same results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (2) Decision Trees for Classification with Count Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_N = 100\n",
    "#convert list of list into text\n",
    "\n",
    "a = df['Review Text'].str.lower().str.cat(sep=' ')\n",
    "\n",
    "# removes punctuation,numbers and returns list of words\n",
    "b = re.sub('[^A-Za-z]+', ' ', a)\n",
    "\n",
    "#remove all the stopwords from the text\n",
    "stop_words = list(get_stop_words('en'))         \n",
    "nltk_words = list(stopwords.words('english'))   \n",
    "stop_words.extend(nltk_words)\n",
    "\n",
    "word_tokens = word_tokenize(b)\n",
    "filtered_sentence = [w for w in word_tokens if not w in stop_words]\n",
    "filtered_sentence = []\n",
    "for w in word_tokens:\n",
    "    if w not in stop_words:\n",
    "        filtered_sentence.append(w)\n",
    "\n",
    "# Remove characters which have length less than 2  \n",
    "without_single_chr = [word for word in filtered_sentence if len(word) > 2]\n",
    "\n",
    "# Remove numbers\n",
    "cleaned_data_title = [word for word in without_single_chr if not word.isnumeric()]  \n",
    "df=df.dropna(axis=0,how='any')\n",
    "rating_class = df[(df['Rating'] == 1) | (df['Rating'] == 5)]\n",
    "X_review=rating_class['Review Text']\n",
    "y=rating_class['Rating']\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "bow_transformer=CountVectorizer(analyzer=text_process).fit(X_review)\n",
    "X_review = bow_transformer.transform(X_review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9651162790697675"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_review, y, test_size=0.3, random_state=101)\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "dtc = DecisionTreeClassifier(criterion=\"entropy\",max_depth=10,min_samples_split=4)\n",
    "dtc.fit(X_train, y_train) \n",
    "dtc.score(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  66  133]\n",
      " [  52 3214]]\n",
      "\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          1       0.56      0.33      0.42       199\n",
      "          5       0.96      0.98      0.97      3266\n",
      "\n",
      "avg / total       0.94      0.95      0.94      3465\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predict = dtc.predict(X_test)\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "print(confusion_matrix(y_test, predict))\n",
    "print('\\n')\n",
    "print(classification_report(y_test, predict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Important thing to note here is, I am restricting how much this tree will grow by keep max_depth = 10\n",
    "\n",
    "### If, I don't do this, this model will overfit. But in order to make an informed decision about max_depth let's do a tuning for this hyperparameter using Grid Search Cross Validation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "parameters = {'max_depth':[i for i in range(10,30)]}\n",
    "gridsearch = GridSearchCV(dtc,param_grid = parameters,scoring='accuracy',cv=10)\n",
    "gridsearch = gridsearch.fit(X_train, y_train) \n",
    "best_accuracy = gridsearch.best_score_\n",
    "best_paramters = gridsearch.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'max_depth': 15}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_paramters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9768678871845621"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_review, y, test_size=0.3, random_state=101)\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "dtc = DecisionTreeClassifier(criterion=\"entropy\",max_depth=best_paramters['max_depth'],\n",
    "                             min_samples_split=3)\n",
    "dtc.fit(X_train, y_train) \n",
    "dtc.score(X_train,y_train)\n",
    "# Just have to make sure that this tuning didn't result into overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  68  131]\n",
      " [  55 3211]]\n",
      "\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          1       0.55      0.34      0.42       199\n",
      "          5       0.96      0.98      0.97      3266\n",
      "\n",
      "avg / total       0.94      0.95      0.94      3465\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predict = dtc.predict(X_test)\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "print(confusion_matrix(y_test, predict))\n",
    "print('\\n')\n",
    "print(classification_report(y_test, predict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Since decision trees have low bias and high variance, they can be enhanced using bootstrap aggregating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9748886689757545"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "bg = BaggingClassifier(base_estimator = dtc, n_estimators = 10)\n",
    "bg.fit(X_train, y_train) \n",
    "bg.score(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  63  136]\n",
      " [  55 3211]]\n",
      "\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          1       0.53      0.32      0.40       199\n",
      "          5       0.96      0.98      0.97      3266\n",
      "\n",
      "avg / total       0.93      0.94      0.94      3465\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predict = bg.predict(X_test)\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "print(confusion_matrix(y_test, predict))\n",
    "print('\\n')\n",
    "print(classification_report(y_test, predict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Important Points to Note\n",
    "\n",
    "1) Although, bagging improved our basic decision tree model, but we didn't get a better result than Naive Bayes\n",
    "\n",
    "2) I tried Adaptive Boost with Decision Tree as base model but nothing changed much\n",
    "\n",
    "3) Random forest (type of bagging technique) was of no help either\n",
    "\n",
    "4) One serious problem with Decision Trees and Ensemble methods (simple bagging, random forest and adaptive bossting) is that they are prone to over fitting, so hyper parameter tuning based on accuracy can give you some major issues when the model is used for out of sample data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  71  128]\n",
      " [  49 3217]]\n",
      "\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          1       0.59      0.36      0.45       199\n",
      "          5       0.96      0.98      0.97      3266\n",
      "\n",
      "avg / total       0.94      0.95      0.94      3465\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "ada = AdaBoostClassifier(base_estimator = dtc, n_estimators = 10)\n",
    "ada.fit(X_train, y_train) \n",
    "predict = ada.predict(X_test)\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "print(confusion_matrix(y_test, predict))\n",
    "print('\\n')\n",
    "print(classification_report(y_test, predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  44  155]\n",
      " [  10 3256]]\n",
      "\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          1       0.81      0.22      0.35       199\n",
      "          5       0.95      1.00      0.98      3266\n",
      "\n",
      "avg / total       0.95      0.95      0.94      3465\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rfc = RandomForestClassifier(n_estimators = 10,random_state = 101)\n",
    "rfc.fit(X_train, y_train) \n",
    "predict = rfc.predict(X_test)\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "print(confusion_matrix(y_test, predict))\n",
    "print('\\n')\n",
    "print(classification_report(y_test, predict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Among these ensemble methods, I am getting better precision - recall using Random Forests\n",
    "\n",
    "#### So, I will not delve into hyperparameter tuning of random forests \n",
    "\n",
    "#### The reason for that is - there are multiple parametes and if tuned perfectly there is a chance of overfitting  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion \n",
    "\n",
    "1) Hyperparameter tuning of Laplace smoothing (alpha = 0.3) in Multinomial Naive Bayes increases the precision by 1%\n",
    "\n",
    "2) Linear Support Vector Classifier gave precision - recall of 97% which is awesome because there was no overfitting in training set. Also, the grid search cross validation helped us to reduce the penalty parameter by 0.1 \n",
    "\n",
    "3) Random Forest gives same precision as Multinomial Naive Bayes but 1% lower recall. However, from my experience I can say that ensemble methods are great (be it bagging or boosting). \n",
    "\n",
    "4) Another thing to note is that Count Vectorization goes well with Multinomial Naive Bayes (MNB) becasue it fulfils the basic assumption of MNB which states that features should be independent of each other. While that's not the case with SVM because, it has to classify using hyperplanes so TF IFD reduces the weight for words that appear a lot in the document. This makes it easier for SVM to look into bigger picture while classifying. \n",
    "\n",
    "5) There is compelling reason as to why discriminative classifiers (such as logistic regression and SVM) are preferred over generative classifiers (such as Naive Bayes). The reason behind this is SVMs solve for posterior probability directly without bothering about the prior probability as an intermediate process. This is the reason that SVM worked really well here. This is something that Vapnik suggested and Andrew Ng published a paper about the comparative analysis. \n",
    "\n",
    "This was the basic motivation of using SVM and Random Forest because they are discriminative models. Usually discriminative classifiers beat generative classifiers when we have huge data set."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
